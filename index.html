<!Doctype html>
<html lang="en">
    <head>
        <title>Shaofei Wang</title>

        <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
        <meta name="author" content="Shaofei Wang">
        <meta name="viewport" content="width=device-width, initial-scale=1">

        <link rel="stylesheet" type="text/css" href="style.css">
        <link href="https://fonts.googleapis.com/css?family=Arvo|Roboto&display=swap" rel="stylesheet">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
        <link rel="stylesheet" href="https://cdn.rawgit.com/jpswalsh/academicons/master/css/academicons.min.css">
    </head>
    <body>
        <div class="section">
            <div class="title">
                <span> Shaofei Wang </span>
            </div>
            <div class="row">
                <div class="image" id="profile-photo">
                    <a>
                    <img alt="Profile photo" src="figures/me.jpeg" />
                    </a>
                </div>
                <div class="content">
                    <p>
                        I am a PhD student at
                        <a href=https://ethz.ch/en.html>
                        ETH Zurich </a>,
                        co-advised by 
                        <a href=https://inf.ethz.ch/people/person-detail.MjYyNzgw.TGlzdC8zMDQsLTg3NDc3NjI0MQ==.html> Prof. Siyu Tang</a>
                        and <a href=http://www.cvlibs.net/> Prof. Andreas Geiger</a>.
                        My research topic is computer vision, specifically in building controllable
                        neural implicit representations for human bodies with clothes. I am also interested
                        in differentiable combinatorial optimization and its application in computer vision.
                        Before I came to ETH, I worked as a researcher in <a href=http://kordinglab.com/> Kording Lab</a>
                        at University of Pennsylvania. Even before that, I worked as a senior system engineer
                        at the autonomous driving group of Baidu.
                        I got my Master's degree from
                        <a href=https://uci.edu/>University of California, Irvine</a>
                        under supervision of 
                        <a href=https://www.ics.uci.edu/~fowlkes/> Prof. Charless Fowlkes</a>
                    </p>
                    <p style="text-align:center">
                        <span class="icon">
                            <a href=mailto:shaofei.wang@inf.ethz.ch>
                                <i class="fa fa-envelope-square fa-2x" aria-hidden="true"></i>
                            </a>
                        </span>
                        <span class="icon">
                            <a href=https://scholar.google.com/citations?user=iz6gEKcAAAAJ&hl=en>
                            <i class="ai ai-google-scholar-square ai-2x"></i>
                            </a> 
                        </span>
                        <span class="icon">
                            <a href=https://github.com/taconite>
                               <i class="fa fa-github-square fa-2x" aria-hidden="true"></i>
                            </a>
                        </span>
			<span class="icon">
                            <a href=https://twitter.com/sfwang0928>
                                <i class="fa fa-twitter-square fa-2x" aria-hidden="true"></i>
                            </a>
                        </span>
                    </p>
                </div>
            </div>
            <div class="title">
                <span>News</span>
            </div>
            <ul class=news_item>
                <li><span class="news-date">03.07.2022:</span><span class="news-text"> Two papers accepted to <a href=https://eccv2022.ecva.net/>ECCV 2022</a>!
                <li><span class="news-date">06.12.2021:</span><span class="news-text"> Code and pre-trained models of MetaAvatar have been released! Please check our <a href="https://neuralbodies.github.io/metavatar/">project page</a> for details.
                <li><span class="news-date">28.09.2021:</span><span class="news-text"> Our paper <a href=https://neuralbodies.github.io/metavatar/>MetaAvatar: Learning Animatable Clothed Human Models from Few Depth Images</a> is accepted to NeurIPS 2021! Code and data will be available soon.
                <li><span class="news-date">21.06.2021:</span><span class="news-text"> Code and pre-trained models of PTF have been released! We also include pre-trained models that work on monocular depth images. Please check our <a href="https://github.com/taconite/PTF">github repository</a> for details.
                <li><span class="news-date">01.03.2021:</span><span class="news-text"> Our paper <a href=https://taconite.github.io/PTF/website/PTF.html>Locally Aware Piecewise Transformation Fields for 3D Human Mesh Registration</a>
                is accepted to CVPR 2021! Code and pre-trained models will be available soon.
                <li><span class="news-date">01.09.2020:</span><span class="news-text"> Started PhD study at <a href=https://ethz.ch/en.html>ETH Zurich</a>.
            </ul>
            <div class="title">
                <span>Publications</span>
            </div>
            <!-- start publication list -->
            <div class="row paper">
                <div class="image">
                    <img src="teasers/ARAH_sq.gif" alt="ARAH: Animatable Volume Rendering of Articulated Human SDFs" />
                </div>
                <div class="content">
                    <div class="paper-title">
                	<a href="https://neuralbodies.github.io/arah/">ARAH: Animatable Volume Rendering of Articulated Human SDFs</a>
                    </div>
                    <div class="conference">European Conference on Computer Vision (ECCV), 2022</div>
                    <div class="authors">
                        <strong class="author">Shaofei Wang</strong>, <a href="https://katjaschwarz.github.io/" class="author">Katja Schwarz</a>, <a href="http://www.cvlibs.net/" class="author">Andreas Geiger</a>, <a href="https://inf.ethz.ch/people/person-detail.MjYyNzgw.TGlzdC8zMDQsLTg3NDc3NjI0MQ==.html" class="author">Siyu Tang</a>
                    </div>
                    <div class="links">
                        <a href="#" data-type="Short" data-index="0">Abstract</a>
                        <a href="https://neuralbodies.github.io/arah/" data-type="Long">Project page</a>
                        <a href="https://arxiv.org/abs/2210.10036" data-type="Short">Paper</a>
                        <a href="https://github.com/taconite/arah-release">Code</a>
                        <a href="#" data-index="4" data-type="Short">Bibtex</a>
                        <div class="link-content" data-index="0">
                            Combining human body models with differentiable rendering has recently enabled animatable avatars of clothed humans from sparse sets of multi-view RGB videos. While state-of-the-art approaches achieve a realistic appearance with neural radiance fields (NeRF), the inferred geometry often lacks detail due to missing geometric constraints. Further, animating avatars in out-of-distribution poses is not yet possible because the mapping from observation space to canonical space does not generalize faithfully to unseen poses. In this work, we address these shortcomings and propose a model to create animatable clothed human avatars with detailed geometry that generalize well to out-of-distribution poses. To achieve detailed geometry, we combine an articulated implicit surface representation with volume rendering. For generalization, we propose a novel joint root-finding algorithm for simultaneous ray-surface intersection search and correspondence search. Our algorithm enables efficient point sampling and accurate point canonicalization while generalizing well to unseen poses. We demonstrate that our proposed pipeline can generate clothed avatars with high-quality pose-dependent geometry and appearance from a sparse set of multi-view RGB videos. Our method achieves state-of-the-art performance on geometry and appearance reconstruction while creating animatable avatars that generalize well to out-of-distribution poses beyond the small number of training poses. 
                        </div>
                        <div class="link-content" data-index="4">
<pre>
@inproceedings{ARAH:ECCV:2022,
  title = {ARAH: Animatable Volume Rendering of Articulated Human SDFs},
  author = {Shaofei Wang and Katja Schwarz and Andreas Geiger and Siyu Tang},
  booktitle = {European Conference on Computer Vision (ECCV)},
  year = {2022}
} 
</pre>
                        </div>
                    </div>
                </div>
            </div>
            <div class="row paper">
                <div class="image">
                    <img src="teasers/COINS.png" alt="COINS" />
                </div>
                <div class="content">
                    <div class="paper-title">
                	<a href="https://zkf1997.github.io/COINS/index.html">COINS: Compositional Human-Scene Interaction Synthesis with Semantic Control</a>
                    </div>
                    <div class="conference">European Conference on Computer Vision (ECCV), 2022</div>
                    <div class="authors">
                        <a href="https://vlg.inf.ethz.ch/team/Kaifeng-Zhao.html" class="author">Kaifeng Zhao</a>, <strong class="author">Shaofei Wang</strong>, <a href="https://yz-cnsdqz.github.io/" class="author">Yan Zhang</a>, <a href="https://thabobeeler.com/" class="author">Thabo Beeler</a>, <a href="https://inf.ethz.ch/people/person-detail.MjYyNzgw.TGlzdC8zMDQsLTg3NDc3NjI0MQ==.html" class="author">Siyu Tang</a>
                    </div>
                    <div class="links">
                        <a href="#" data-type="Short" data-index="0">Abstract</a>
                        <a href="https://zkf1997.github.io/COINS/index.html" data-type="Long">Project page</a>
                        <a href="https://arxiv.org/abs/2207.12824" data-type="Short">Paper</a>
                        <a href="https://github.com/zkf1997/COINS">Code</a>
                        <a href="#" data-index="4" data-type="Short">Bibtex</a>
                        <div class="link-content" data-index="0">
                            Our goal is to synthesize humans interacting with a given 3D scene controlled by high-level semantic specifications as pairs of action categories and object instances, e.g., “sit on the chair”. The key challenge of incorporating interaction semantics into the generation framework is to learn a joint representation that effectively captures heterogeneous information, including human body articulation, 3D object geometry, and the intent of the interaction. To address this challenge, we design a novel transformer-based generative model, in which the articulated 3D human body surface points and 3D objects are jointly encoded in a unified latent space, and the semantics of the interaction between the human and objects are embedded via positional encoding. Furthermore, inspired by the compositional nature of interactions that humans can simultaneously interact with multiple objects, we define interaction semantics as the composition of varying numbers of atomic action-object pairs. Our proposed generative model can naturally incorporate varying numbers of atomic interactions, which enables synthesizing compositional human-scene interactions without requiring composite interaction data. 
                        </div>
                        <div class="link-content" data-index="4">
<pre>
@inproceedings{Zhao:ECCV:2022,
  title = {Compositional Human-Scene Interaction Synthesis with Semantic Control},
  author = {Zhao, Kaifeng and Wang, Shaofei and Zhang, Yan and Beeler, Thabo and and Tang, Siyu},
  booktitle = {European conference on computer vision (ECCV)},
  month = oct,
  year = {2022}
} 
</pre>
                        </div>
                    </div>
                </div>
            </div>
            <div class="row paper">
                <div class="image">
                    <img src="teasers/MetaAvatar.jpeg" alt="MetaAvatar: Learning Animatable Clothed Human Models from Few Depth Images" />
                </div>
                <div class="content">
                    <div class="paper-title">
                	<a href="https://neuralbodies.github.io/metavatar/">MetaAvatar: Learning Animatable Clothed Human Models from Few Depth Images</a>
                    </div>
                    <div class="conference">Neural Information Process Systems (NeurIPS), 2021</div>
                    <div class="authors">
                        <strong class="author">Shaofei Wang</strong>, <a href="https://markomih.github.io/" class="author">Marko Mihajlovic</a>, <a href="https://qianlim.github.io/" class="author">Qianli Ma</a>, <a href="http://www.cvlibs.net/" class="author">Andreas Geiger</a>, <a href="https://inf.ethz.ch/people/person-detail.MjYyNzgw.TGlzdC8zMDQsLTg3NDc3NjI0MQ==.html" class="author">Siyu Tang</a>
                    </div>
                    <div class="links">
                        <a href="#" data-type="Short" data-index="0">Abstract</a>
                        <a href="https://neuralbodies.github.io/metavatar/" data-type="Long">Project page</a>
                        <a href="https://arxiv.org/abs/2106.11944" data-type="Short">Paper</a>
                        <a href="https://github.com/taconite/MetaAvatar-release">Code</a>
                        <a href="#" data-index="4" data-type="Short">Bibtex</a>
                        <div class="link-content" data-index="0">
                            In this paper, we aim to create generalizable and controllable neural signed distance fields (SDFs) that represent clothed humans from monocular depth observations. Recent advances in deep learning, especially neural implicit representations, have enabled human shape reconstruction and controllable avatar generation from different sensor inputs. However, to generate realistic cloth deformations from novel input poses, watertight meshes or dense full-body scans are usually needed as inputs. Furthermore, due to the difficulty of effectively modeling pose-dependent cloth deformations for diverse body shapes and cloth types, existing approaches resort to per-subject/cloth-type optimization from scratch, which is computationally expensive. In contrast, we propose an approach that can quickly generate realistic clothed human avatars, represented as controllable neural SDFs, given only monocular depth images. We achieve this by using meta-learning to learn an initialization of a hypernetwork that predicts the parameters of neural SDFs. The hypernetwork is conditioned on human poses and represents a clothed neural avatar that deforms non-rigidly according to the input poses. Meanwhile, it is meta-learned to effectively incorporate priors of diverse body shapes and cloth types and thus can be much faster to fine-tune, compared to models trained from scratch. We qualitatively and quantitatively show that our approach outperforms state-of-the-art approaches that require complete meshes as inputs while our approach requires only depth frames as inputs and runs orders of magnitudes faster. Furthermore, we demonstrate that our meta-learned hypernetwork is very robust, being the first to generate avatars with realistic dynamic cloth deformations given as few as 8 monocular depth frames. 
                        </div>
                        <div class="link-content" data-index="4">
<pre>
@inproceedings{MetaAvatar:NeurIPS:2021,
  author = {Shaofei Wang and Marko Mihajlovic and Qianli Ma and Andreas Geiger and Siyu Tang},
  title = {MetaAvatar: Learning Animatable Clothed Human Models from Few Depth Images},
  booktitle = {Conference on Neural Information Processing Systems (NeurIPS)},
  year = {2021}
} 
</pre>
	                </div>
	            </div>
	        </div>
	    </div>
            <div class="row paper">
                <div class="image"><img src="teasers/PTF.png" alt="Locally Aware Piecewise Transformation Fields for 3D Human Mesh Registration" /></div>
                <div class="content">
                    <div class="paper-title">
                        <a href="https://taconite.github.io/PTF/website/PTF.html">Locally Aware Piecewise Transformation Fields for 3D Human Mesh Registration</a>
                    </div>
                    <div class="conference">Computer Vision and Pattern Recognition (CVPR), 2021 </div>
                    <div class="authors">
                        <strong class="author">Shaofei Wang</strong>, <a href="http://www.cvlibs.net/" class="author">Andreas Geiger</a>, <a href="https://inf.ethz.ch/people/person-detail.MjYyNzgw.TGlzdC8zMDQsLTg3NDc3NjI0MQ==.html" class="author">Siyu Tang</a>
                    </div>
                    <div class="links">
                        <a href="#" data-type="Short" data-index="0">Abstract</a>
                        <a href="https://taconite.github.io/PTF/website/PTF.html" data-type="Long">Project page</a>
                        <a href="https://arxiv.org/abs/2104.08160" data-type="Short">Paper</a>
                        <a href="https://github.com/taconite/PTF">Code</a>
                        <a href="#" data-index="4" data-type="Short">Bibtex</a>
                        <div class="link-content" data-index="0">
                            Registering point clouds of dressed humans to parametric human models is a challenging task in computer vision. Traditional approaches often rely on heavily engineered pipelines that require accurate manual initialization of human poses and tedious post-processing. More recently, learning-based methods are proposed in hope to automate this process. We observe that pose initialization is key to accurate registration but existing methods often fail to provide accurate pose initialization. One major obstacle is that, despite recent effort on rotation representation learning in neural networks, regressing joint rotations from point clouds or images of humans is still very challenging. To this end, we propose novel piecewise transformation fields (PTF), a set of functions that learn 3D translation vectors to map any query point in posed space to its correspond position in rest-pose space. We combine PTF with multi-class occupancy networks, obtaining a novel learning-based framework that learns to simultaneously predict shape and per-point correspondences between the posed space and the canonical space for clothed human. Our key insight is that the translation vector for each query point can be effectively estimated using the point-aligned local features; consequently, rigid per bone transformations and joint rotations can be obtained efficiently via a least-square fitting given the estimated point correspondences, circumventing the challenging task of directly regressing joint rotations from neural networks. Furthermore, the proposed PTF facilitate canonicalized occupancy estimation, which greatly improves generalization capability and results in more accurate surface reconstruction with only half of the parameters compared with the state-of-the-art. Both qualitative and quantitative studies show that fitting parametric models with poses initialized by our network results in much better registration quality, especially for extreme poses.
                        </div>
                        <div class="link-content" data-index="4">
<pre>
@inproceedings{PTF:CVPR:2021,
  author = {Shaofei Wang and Andreas Geiger and Siyu Tang},
  title = {Locally Aware Piecewise Transformation Fields for 3D Human Mesh Registration},
  booktitle = {Conference on Computer Vision and Pattern Recognition (CVPR)},
  year = {2021}
}
</pre>
                        </div>
                    </div>
                </div>
            </div>
            <div class="row paper">
                <div class="image"><img src="teasers/entity_resolve.png" alt="Accelerating Column Generation via Flexible Dual Optimal Inequalities with Application to Entity Resolution" /></div>
                <div class="content">
                    <div class="paper-title">
                        <a href="https://github.com/lokhande-vishnu/EntityResolution">Accelerating Column Generation via Flexible Dual Optimal Inequalities with Application to Entity Resolution</a> <a><b>(Oral)</b></a>
                    </div>
                    <div class="conference">AAAI Conference on Artificial Intelligence (AAAI), 2020</div>
                    <div class="authors">
                        <a href="https://lokhande-vishnu.github.io/" class="author">Vishnu Suresh Lokhande</a>, <strong class="author">Shaofei Wang</strong>, <a href="https://www.linkedin.com/in/maneesh-singh-3523ab9/" class="author">Maneesh Singh</a>, <a href="https://sites.google.com/site/julianyarkonymachinelearning/" class="author">Julian Yarkony</a>
                    </div>
                    <div class="links">
                        <a href="#" data-type="Short" data-index="0">Abstract</a>
                        <a href="https://arxiv.org/abs/1909.05460" data-type="Short">Paper</a>
                        <a href="https://github.com/lokhande-vishnu/EntityResolution">Code</a>
                        <a href="#" data-index="4" data-type="Short">Bibtex</a>
                        <div class="link-content" data-index="0">
                           In this paper, we introduce a new optimization approach to Entity Resolution. Traditional approaches tackle entity resolution with hierarchical clustering, which does not benefit from a formal optimization formulation. In contrast, we model entity resolution as correlation-clustering, which we treat as a weighted set-packing problem and write as an integer linear program (ILP). In this case sources in the input data correspond to elements and entities in output data correspond to sets/clusters. We tackle optimization of weighted set packing by relaxing integrality in our ILP formulation. The set of potential sets/clusters can not be explicitly enumerated, thus motivating optimization via column generation. In addition to the novel formulation, we also introduce new dual optimal inequalities (DOI), that we call flexible dual optimal inequalities, which tightly lower-bound dual variables during optimization and accelerate column generation. We apply our formulation to entity resolution (also called de-duplication of records), and achieve state-of-the-art accuracy on two popular benchmark datasets. 
                        </div>
                        <div class="link-content" data-index="4">
<pre>
@inproceedings{EntityRes:AAAI:2020,
  author = {Vishnu Suresh Lokhande, Shaofei Wang, Maneesh Singh, Julian Yarkony},
  title = {Accelerating Column Generation via Flexible Dual Optimal Inequalities with Application to Entity Resolution},
  booktitle = {AAAI Conference on Artificial Intelligence (AAAI)},
  year = {2020}
}
</pre>
                        </div>
                    </div>
                </div>
            </div>
            <div class="row paper">
                <div class="image"><img src="teasers/benders.png" alt="Accelerating Dynamic Programs via Nested Benders Decomposition with Application to Multi-person Pose Estimation" /></div>
                <div class="content">
                    <div class="paper-title">
                        <a>Accelerating Dynamic Programs via Nested Benders Decomposition with Application to Multi-person Pose Estimation</a>
                    </div>
                    <div class="conference">European Conference on Computer Vision (ECCV), 2018</div>
                    <div class="authors">
                        <strong class="author">Shaofei Wang</strong>, <a href="https://www.ics.uci.edu/~ihler/" class="author">Alexander Ihler</a>, <a href="http://kordinglab.com/" class="author">Konrad Kording</a>, <a href="https://sites.google.com/site/julianyarkonymachinelearning/" class="author">Julian Yarkony</a>
                    </div>
                    <div class="links">
                        <a href="#" data-type="Short" data-index="0">Abstract</a>
                        <a href="https://openaccess.thecvf.com/content_ECCV_2018/papers/Shaofei_Wang_Accelerating_Dynamic_Programs_ECCV_2018_paper.pdf" data-type="Short">Paper</a>
                        <a href="#" data-index="4" data-type="Short">Bibtex</a>
                        <div class="link-content" data-index="0">
                           We present a novel approach to solve dynamic programs (DP), which are frequent in computer vision, on tree-structured graphs with exponential node state space. Typical DP approaches have to enumerate the joint state space of two adjacent nodes on every edge of the tree to compute the optimal messages. Here we propose an algorithm based on Nested Benders Decomposition (NBD) which iteratively lower-bounds the message on every edge and promises to be far more efficient. We apply our NBD algorithm along with a novel Minimum Weight Set Packing (MWSP) formulation to a multi-person pose estimation problem. While our algorithm is provably optimal at termination it operates in linear time for practical DP problems, gaining up to 500x speed up over traditional DP algorithm which have polynomial complexity. 
                        </div>
                        <div class="link-content" data-index="4">
<pre>
@inproceedings{Benders:ECCV:2018,
  author = {Shaofei Wang, Alexander Ihler, Konrad Kording, Julian Yarkony},
  title = {Accelerating Dynamic Programs via Nested Benders Decomposition with Application to Multi-person Pose Estimation},
  booktitle = {European Conference on Computer Vision (ECCV)},
  year = {2018}
}
</pre>
                        </div>
                    </div>
                </div>
            </div>
            <div class="row paper">
                <div class="image"><img src="teasers/learn_mot_ijcv.png" alt="Learning Optimal Parameters for Multi-target Tracking with Contextual Interactions" /></div>
                <div class="content">
                    <div class="paper-title">
                        <a>Learning Optimal Parameters for Multi-target Tracking with Contextual Interactions</a>
                    </div>
                    <div class="conference">International Journal of Computer Vision (IJCV), 2017</div>
                    <div class="authors">
                        <strong class="author">Shaofei Wang</strong>, <a href="https://www.ics.uci.edu/~fowlkes/" class="author">Charless C. Fowlkes</a>
                    </div>
                    <div class="links">
                        <a href="#" data-type="Short" data-index="0">Abstract</a>
                        <a href="https://arxiv.org/abs/1610.01394" data-type="Short">Paper</a>
                        <a href="#" data-index="4" data-type="Short">Bibtex</a>
                        <div class="link-content" data-index="0">
                           We describe an end-to-end framework for learning parameters of min-cost flow multi-target tracking problem with quadratic trajectory interactions including suppression of overlapping tracks and contextual cues about cooccurrence of different objects. Our approach utilizes structured prediction with a tracking-specific loss function to learn the complete set of model parameters. In this learning framework, we evaluate two different approaches to finding an optimal set of tracks under a quadratic model objective, one based on an LP relaxation and the other based on novel greedy variants of dynamic programming that handle pairwise interactions. We find the greedy algorithms achieve almost equivalent accuracy to the LP relaxation while being up to 10x faster than a commercial LP solver. We evaluate trained models on three challenging benchmarks. Surprisingly, we find that with proper parameter learning, our simple data association model without explicit appearance/motion reasoning is able to achieve comparable or better accuracy than many state-of-the-art methods that use far more complex motion features or appearance affinity metric learning.
                        </div>
                        <div class="link-content" data-index="4">
<pre>
@article{learn_mot:IJCV:2017,
  author = {Shaofei Wang, Charless C. Fowlkes},
  title = {Learning Optimal Parameters for Multi-target Tracking with Contextual Interactions},
  journal = {International Journal of Computer Vision (IJCV)},
  volume = {122},
  pages = {484–501},
  year = {2017}
}
</pre>
                        </div>
                    </div>
                </div>
            </div>
            <div class="row paper">
                <div class="image"><img src="teasers/dcg_mot.png" alt="Tracking Objects with Higher Order Interactions via Delayed Column Generation" /></div>
                <div class="content">
                    <div class="paper-title">
                        <a>Tracking Objects with Higher Order Interactions via Delayed Column Generation</a>
                    </div>
                    <div class="conference">International Conference on Artificial Intelligence and Statistics (AISTATS), 2017</div>
                    <div class="authors">
                        <strong class="author">Shaofei Wang</strong>, <a href="https://steffenwolf.science/" class="author"> Steffen Wolf</a>, <a href="https://www.ics.uci.edu/~fowlkes/" class="author">Charless C. Fowlkes</a>, <a href="https://sites.google.com/site/julianyarkonymachinelearning/" class="author">Julian Yarkony</a>
                    </div>
                    <div class="links">
                        <a href="#" data-type="Short" data-index="0">Abstract</a>
                        <a href="http://proceedings.mlr.press/v54/wang17c.html" data-type="Short">Paper</a>
                        <a href="#" data-index="4" data-type="Short">Bibtex</a>
                        <div class="link-content" data-index="0">
                            We study the problem of multi-target tracking and data association in video. We formulate this in terms of selecting a subset of high-quality tracks subject to the constraint that no pair of selected tracks is associated with a common detection (of an object). This objective is equivalent to the classic NP-hard problem of finding a maximum-weight set packing (MWSP) where tracks correspond to sets and is made further difficult since the number of candidate tracks grows exponentially in the number of detections. We present a relaxation of this combinatorial problem that uses a column generation formulation where the pricing problem is solved via dynamic programming to efficiently explore the space of tracks. We employ row generation to tighten the bound in such a way as to preserve efficient inference in the pricing problem. We show the practical utility of this algorithm for pedestrian and particle tracking. 
                        </div>
                        <div class="link-content" data-index="4">
<pre>
@inproceedings{dcg_mot:AISTATS:2017,
  author = {Shaofei Wang, Steffen Wolf, Charless C. Fowlkes, Julian Yarkony},
  title = {Tracking Objects with Higher Order Interactions via Delayed Column Generation},
  journal = {International Conference on Artificial Intelligence and Statistics (AISTATS)},
  year = {2017}
}
</pre>
                        </div>
                    </div>
                </div>
            </div>
            <div class="row paper">
                <div class="image"><img src="teasers/learn_mot_bmvc.png" alt="Learning Optimal Parameters For Multi-target Tracking" /></div>
                <div class="content">
                    <div class="paper-title">
                        <a>Learning Optimal Parameters For Multi-target Tracking</a> <a><b>(Oral)</b></a>
                    </div>
                    <div class="conference">British Machine Vision Conference (BMVC), 2015</div>
                    <div class="authors">
                        <strong class="author">Shaofei Wang</strong>, <a href="https://www.ics.uci.edu/~fowlkes/" class="author">Charless C. Fowlkes</a></a>
                    </div>
                    <div class="links">
                        <a href="#" data-type="Short" data-index="0">Abstract</a>
                        <a href="https://www.ics.uci.edu/~fowlkes/papers/wf-multitarget-bmvc2015.pdf" data-type="Short">Paper</a>
                        <a href="#" data-index="4" data-type="Short">Bibtex</a>
                        <div class="link-content" data-index="0">
                            We describe an end-to-end framework for learning parameters of min-cost flow multitarget tracking problem with quadratic trajectory interactions including suppression of overlapping tracks and contextual cues about co-occurrence of different objects. Our approach utilizes structured prediction with a tracking-specific loss function to learn the complete set of model parameters. Under our learning framework, we evaluate two different approaches to finding an optimal set of tracks under quadratic model objective based on an LP relaxation and a novel greedy extension to dynamic programming that handles pairwise interactions. We find the greedy algorithm achieves almost equivalent accuracy to the LP relaxation while being 2-7x faster than a commercial solver. We evaluate trained models on the challenging MOT and KITTI benchmarks. Surprisingly, we find that with proper parameter learning, our simple data-association model without explicit appearance/motion reasoning is able to outperform many state-of-the-art methods that use far more complex motion features and affinity metric learning.
                        </div>
                        <div class="link-content" data-index="4">
<pre>
@inproceedings{learn_mot:BMVC:2015,
  author = {Shaofei Wang, Charless C. Fowlkes},
  title = {Learning Optimal Parameters For Multi-target Tracking},
  journal = {British Machine Vision Conference (BMVC)},
  year = {2015}
}
</pre>
                        </div>
                    </div>
                </div>
            </div>
<!-- end publication list -->
        <!-- Javascript for showing and hiding the abstract and bibtex -->
        <script type="text/javascript">
            document.querySelectorAll(".links").forEach(function (p) {
                p.addEventListener("click", function (ev) {
                    // Make sure that the click is coming from a link
                    if (ev.target.nodeName != "A") {
                        return;
                    }

                    // Find the index of the div to toggle or return
                    var i = ev.target.dataset["index"];
                    if (i == undefined) {
                        return;
                    }

                    // Make sure to remove something else that was displayed
                    // and toggle the current one
                    Array.prototype.forEach.call(
                        ev.target.parentNode.children,
                        function (sibling) {
                            // We don't care about links etc
                            if (sibling.nodeName != "DIV") {
                                return;
                            }

                            // Hide others
                            if (sibling.dataset["index"] != i) {
                                sibling.style.display = "none";
                            }

                            // toggle the correct one
                            else {
                                if (sibling.style.display != "block") {
                                    sibling.style.display = "block";
                                } else {
                                    sibling.style.display = "none";
                                }
                            }
                        }
                    );
                    ev.preventDefault();
                });
            });
        </script>

        <!-- Global site tag (gtag.js) - Google Analytics -->
        <script async src="https://www.googletagmanager.com/gtag/js?id=UA-143288088-1"></script>
        <script>
            window.dataLayer = window.dataLayer || [];
            function gtag(){dataLayer.push(arguments);}
            gtag('js', new Date());

            gtag('config', 'UA-143288088-1');
        </script>
    </body>
</html>
